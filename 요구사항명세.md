# 웹 스크래핑 자동화 빌더 - 요구사항 명세서

> 작성일: 2025-11-30  
> 상태: 초안 (설계 진행 중)
---

## 1. 프로젝트 개요

### 1.1 비전
"드래그 앤 드롭으로 웹 스크래핑 봇을 만들고, 매일 자동으로 데이터를 수집해서 Google Sheets/Slack에 전송"

### 1.2 타겟 사용자
- 비개발자: 경쟁사 가격 모니터링이 필요한 마케터/기획자
- 소규모 비즈니스: 부동산 데이터, 이커머스 상품 추적이 필요한 사업자
- 데이터 분석가: 반복적인 데이터 수집 자동화가 필요한 분석가

### 1.3 수익 모델
| 플랜 | 가격 | 제한 | 주요 기능 |
|------|------|------|-----------|
| Starter | $29/월 | 1,000행/월 | 기본 스크래핑, 스케줄링 |
| Pro | $99/월 | 무제한 | API 연동, 고급 셀렉터 |
| Enterprise | $499/월 | 무제한 | 커스텀 워크플로우, 전담 지원 |

---

## 2. 핵심 기능 요구사항

### 2.1 봇 빌더 (No-Code Editor)

#### 2.1.1 비주얼 셀렉터
- [ ] 브라우저 확장 프로그램 또는 내장 브라우저로 웹페이지 렌더링
- [ ] 클릭으로 요소 선택 → CSS/XPath 셀렉터 자동 생성
- [ ] 선택한 요소 하이라이트 및 미리보기
- [ ] 페이지네이션 패턴 자동 감지

#### 2.1.2 워크플로우 빌더
- [ ] 드래그 앤 드롭 액션 블록
  - 페이지 이동 (Navigate)
  - 클릭 (Click)
  - 텍스트 입력 (Type)
  - 데이터 추출 (Extract)
  - 대기 (Wait)
  - 조건 분기 (If/Else)
  - 반복 (Loop)
- [ ] 워크플로우 저장/불러오기
- [ ] 템플릿 라이브러리 (인기 사이트용 프리셋)

### 2.2 스크래핑 엔진

#### 2.2.1 실행 방식
- [ ] 헤드리스 브라우저 기반 (Puppeteer/Playwright)
- [ ] 동적 콘텐츠(JavaScript 렌더링) 지원
- [ ] 프록시 로테이션
- [ ] User-Agent 랜덤화
- [ ] 캡챠 우회 (🔴 확정 필요)

#### 2.2.2 스케줄링
- [ ] Cron 기반 스케줄 설정
- [ ] 시간대(Timezone) 지원
- [ ] 수동 즉시 실행
- [ ] 실행 이력 및 로그 조회

### 2.3 데이터 처리 및 내보내기

#### 2.3.1 데이터 변환
- [ ] 추출 데이터 정제 (공백 제거, 포맷팅)
- [ ] 정규식 기반 파싱
- [ ] 중복 제거
- [ ] 데이터 타입 변환 (숫자, 날짜 등)

#### 2.3.2 내보내기 연동
- [ ] Google Sheets 연동
- [ ] Slack 알림
- [ ] Webhook (커스텀 API 호출)
- [ ] CSV/JSON 다운로드
- [ ] 🔴 추가 연동 (Notion, Airtable 등 - 확정 필요)

### 2.4 사용자 관리

#### 2.4.1 인증
- [ ] 이메일/비밀번호 로그인
- [ ] OAuth (Google, GitHub)
- [ ] 팀 멤버 초대 (Pro 이상)

#### 2.4.2 사용량 관리
- [ ] 월간 행 수 카운팅
- [ ] 사용량 대시보드
- [ ] 한도 초과 알림
- [ ] 결제/구독 관리 (Stripe 연동)

---

## 3. 시스템 아키텍처 (초안)

```
┌─────────────────────────────────────────────────────────────────┐
│                        Frontend                                  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │ 봇 빌더 UI  │  │  대시보드   │  │  결제 페이지 │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                        Backend API                               │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │  봇 CRUD    │  │ 스케줄 관리 │  │  사용자/결제 │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                     Worker Queue                                 │
│  ┌─────────────────────────────────────────────────┐            │
│  │  스크래핑 작업 큐 (Redis/BullMQ)                 │            │
│  └─────────────────────────────────────────────────┘            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                   Scraping Workers                               │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐            │
│  │ Worker1 │  │ Worker2 │  │ Worker3 │  │ Worker N │            │
│  │(Browser)│  │(Browser)│  │(Browser)│  │(Browser)│            │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Storage                                     │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │  PostgreSQL │  │    Redis    │  │  S3/Storage │              │
│  │ (메인 DB)   │  │  (캐시/큐)  │  │ (스크린샷)  │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└─────────────────────────────────────────────────────────────────┘
```

---

## 4. 기술 스택 (🔴 확정 필요)

| 영역 | 후보 기술 | 비고 |
|------|-----------|------|
| Frontend | React + Next.js / Vue + Nuxt | ? |
| 봇 빌더 UI | React Flow / Vue Flow | 드래그앤드롭 |
| Backend | Node.js (Express/Fastify) / Python (FastAPI) | ? |
| 스크래핑 엔진 | Puppeteer / Playwright | Playwright 권장 |
| 작업 큐 | BullMQ (Redis) / Celery | ? |
| DB | PostgreSQL | 확정 |
| 캐시/세션 | Redis | 확정 |
| 인프라 | AWS / GCP / Vercel + Railway | ? |
| 결제 | Stripe | 확정 |

---

## 5. 병목 예상 지점 및 최적화 전략

### 5.1 🔴 병목 지점 1: 브라우저 인스턴스 관리
**문제**: 헤드리스 브라우저는 메모리/CPU 집약적
- 동시 실행 브라우저 수 제한 필요
- 브라우저 크래시 시 복구 전략

**최적화 방안**:
- 브라우저 풀(Pool) 관리 - 재사용으로 초기화 비용 절감
- 컨테이너 기반 격리 (Docker)
- Auto-scaling 워커

### 5.2 🔴 병목 지점 2: 스케줄 동시 실행
**문제**: 많은 사용자가 같은 시간(예: 매일 오전 9시)에 스케줄 설정

**최적화 방안**:
- 작업 큐로 부하 분산
- 스케줄 시간 분산 유도 (UI에서 "추천 시간" 제안)
- 우선순위 큐 (유료 플랜 우선)

### 5.3 🔴 병목 지점 3: 타겟 사이트 차단
**문제**: 동일 IP로 다수 요청 시 차단

**최적화 방안**:
- 프록시 풀 관리 (Residential/Datacenter)
- 요청 속도 제한 (Rate Limiting)
- 지능형 재시도 (Exponential Backoff)

### 5.4 🔴 병목 지점 4: 대용량 데이터 처리
**문제**: 수천~수만 행 추출 시 메모리 이슈

**최적화 방안**:
- 스트리밍 처리 (청크 단위)
- 백그라운드 내보내기 (완료 시 알림)
- 결과 페이지네이션

---

## 6. 확정된 결정 사항

### ✅ 결정 1: 비주얼 셀렉터 구현 방식
**결정: 둘 다 구현 (Freemium 전략)**

| 티어 | 방식 | 이유 |
|------|------|------|
| 무료/체험 | iframe 기반 | 설치 없이 즉시 체험 → 진입장벽 ↓ |
| 유료 | Chrome 확장 프로그램 | 풀 기능, 안정적 UX → 결제 유도 |

장점:
- 직접 두 방식을 비교 체험 가능
- iframe의 한계를 느낀 사용자가 자연스럽게 유료 전환
- 나중에 iframe 제거해도 됨 (실험적 접근)

---

### ✅ 결정 2: MVP 범위
**원칙: "사용자가 기꺼이 돈을 낼 만한 가치" → 결제까지 완성**

MVP 필수 기능:
- [x] iframe 기반 셀렉터 (무료)
- [x] Chrome 확장 프로그램 기반 셀렉터 (유료)
- [x] 기본 스크래핑 (단일/다중 페이지)
- [x] **페이지네이션 + 목록 순회** (필수)
- [x] 스케줄링 (자동 수집)
- [x] Google Sheets 연동
- [x] 사용자 인증
- [x] 결제 시스템

---

### ✅ 결정 3: 타겟 시장 전략
**2단계 전략:**
1. **Phase 1**: 한국어 UI로 시작 → 첫 유료 고객 확보
2. **Phase 2**: 첫 결제 발생 시 → 글로벌(영어) 마이그레이션

결제 시스템:
- Phase 1: 토스페이먼츠 (한국)
- Phase 2: Stripe (글로벌) 추가

---

### ✅ 결정 4: 스크래핑 엔진 전략
**Firecrawl + LLM 하이브리드 접근**

```
┌─────────────────────────────────────────────────────────────────┐
│                      스크래핑 파이프라인                         │
│                                                                  │
│  ┌─────────┐    ┌─────────────┐    ┌─────────────┐    ┌──────┐ │
│  │ 사용자  │    │  Firecrawl  │    │  LLM 전처리 │    │ 결과 │ │
│  │ 요청    │ ─▶ │  (원본수집) │ ─▶ │  (정제/추출)│ ─▶ │ 저장 │ │
│  └─────────┘    └─────────────┘    └─────────────┘    └──────┘ │
│                                                                  │
│  "이 사이트에서      전체 HTML/      "가격만 추출해"    구조화된 │
│   가격 정보 수집"    마크다운 수집    "테이블로 변환"    데이터   │
└─────────────────────────────────────────────────────────────────┘
```

**핵심 아이디어:**
- Firecrawl: 원본 데이터 대량 수집 (URL → Raw Data)
- LLM: 수집된 데이터를 사용자 의도에 맞게 정제/추출
- Function Calling: Firecrawl MCP를 LLM 도구로 연결

---

### ✅ 결정 5: 최종 목표 (North Star)
**Playwright MCP 기반 화면 직접 추출**

> "화면에서 직접 데이터를 추출하는 방식"
> → 복잡도 ↑ 하지만 UX 극대화

**로드맵:**
```
Phase 1 (MVP)          Phase 2               Phase 3 (최종)
─────────────         ─────────────         ─────────────
Firecrawl +           + 고급 셀렉터         Playwright MCP
LLM 전처리            + 인터랙션 녹화       화면 직접 추출
                                            실시간 미리보기
```

---

## 7. 경쟁사 분석 (🔴 조사 필요)

### 7.1 조사 대상 후보
| 서비스 | 유형 | 확인 포인트 |
|--------|------|-------------|
| **Octoparse** | 데스크톱 앱 | 가격, 기능 범위, UX |
| **ParseHub** | 데스크톱 앱 | 무료 티어 제한 |
| **Apify** | 클라우드 | 개발자 타겟, 가격 |
| **Browse AI** | 노코드 SaaS | 가장 유사, 핵심 벤치마크 |
| **Bardeen** | 브라우저 확장 | 자동화 접근법 |
| **Instant Data Scraper** | 확장 프로그램 | 무료 대안 |

### 7.2 분석 프레임워크
각 경쟁사에 대해:
1. **가격 정책**: 무료 티어, 유료 시작가, 엔터프라이즈
2. **핵심 기능**: 셀렉터 방식, 스케줄링, 내보내기
3. **차별점**: 무엇이 특별한가?
4. **약점**: 사용자 불만, 리뷰에서 언급되는 문제
5. **우리의 기회**: 어디서 이길 수 있나?

### 🔴 TODO: 경쟁사 실제 사용해보고 분석 결과 정리

---

## 8. 기술 검토: Firecrawl + LLM 아키텍처

### 8.1 Firecrawl이란?
- 오픈소스 웹 스크래핑/크롤링 API
- 자체 호스팅(Self-hosted) 가능
- JavaScript 렌더링 지원, 마크다운 변환 등 제공

### 8.2 새로운 아키텍처: Firecrawl + LLM 파이프라인

```
┌─────────────────────────────────────────────────────────────────┐
│                     사용자 인터페이스                            │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  "이 쇼핑몰에서 상품명, 가격, 재고 상태를 수집해줘"      │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    LLM Orchestrator                              │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  Function Calling으로 도구 선택 및 실행                  │   │
│  │  - firecrawl_scrape(url)                                 │   │
│  │  - firecrawl_crawl(url, options)                         │   │
│  │  - extract_data(html, schema)                            │   │
│  │  - transform_data(data, format)                          │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                   Firecrawl MCP (Self-hosted)                    │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │   Scrape    │  │    Crawl    │  │    Map      │              │
│  │ (단일 URL)  │  │ (사이트전체)│  │ (URL 발견) │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    LLM 데이터 전처리                             │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  Raw HTML/Markdown → 구조화된 JSON                       │   │
│  │  - 스키마 추론 (사용자가 원하는 필드 자동 감지)          │   │
│  │  - 노이즈 제거 (광고, 네비게이션 등)                     │   │
│  │  - 데이터 정규화 (가격 포맷, 날짜 등)                    │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Output                                      │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │ Google      │  │   Slack     │  │  Webhook    │              │
│  │ Sheets      │  │   알림      │  │  (API)      │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
└─────────────────────────────────────────────────────────────────┘
```

### 8.3 이 접근법의 차별점

**기존 노코드 스크래퍼:**
```
사용자가 직접 CSS 셀렉터 지정 → 정확히 그 요소만 추출
→ 문제: 사이트 구조 바뀌면 깨짐, 학습 곡선 존재
```

**우리의 LLM 기반 접근:**
```
사용자가 자연어로 요청 → LLM이 알아서 추출
→ 장점: 사이트 구조 변경에 유연, 비개발자 친화적
→ 단점: LLM 비용, 정확도 보장 어려움
```

### 8.4 하이브리드 전략
| 모드 | 사용 시점 | 방식 |
|------|----------|------|
| **AI 자동 모드** | 빠른 시작, 간단한 추출 | LLM이 자동 추론 |
| **정밀 모드** | 정확도 중요, 반복 작업 | 사용자가 셀렉터 지정 |
| **학습 모드** | AI 추출 후 | 사용자 피드백으로 개선 |

---

## 9. 기술 스택 결정

### ✅ 확정된 스택

| 영역 | 기술 | 이유 |
|------|------|------|
| **Frontend** | React + Next.js + TypeScript | 익숙함, 생태계 |
| **Backend** | FastAPI (Python) | 비동기 처리, LLM 통합 용이 |
| **LLM** | OpenRouter (무료 모델) → GPT-5 (Agent 필요시) | 비용 효율 |
| **DB** | PostgreSQL | 확정 |
| **캐시/세션** | Redis | 확정 |
| **작업 큐** | ✅ Celery | Python 통일, 워크플로우 체이닝 |
| **인프라** | Vercel + Railway → 추후 마이그레이션 | 빠른 시작 |
| **결제** | 토스페이먼츠 → Stripe | Phase별 |

### 9.1 React+Next.js vs Vue+Nuxt 비교

| 항목 | React + Next.js | Vue + Nuxt |
|------|-----------------|------------|
| **학습 곡선** | 가파름 (JSX, 훅 패턴) | 완만함 (템플릿 문법) |
| **유연성** | ✅ 높음 (자유도 큼) | 중간 (규약 있음) |
| **생태계** | ✅ 가장 큼 | 크지만 React보다 작음 |
| **TypeScript** | ✅ 네이티브 수준 지원 | 좋음, 하지만 React가 더 성숙 |
| **성능** | 비슷함 | 비슷함 |
| **채용 시장** | ✅ 압도적으로 큼 | 작음 |
| **복잡한 UI** | ✅ 봇 빌더 같은 복잡한 UI에 유리 | 가능하지만 라이브러리 적음 |
| **드래그앤드롭** | React Flow ✅ | Vue Flow (포크, 덜 성숙) |

**결론**: 이미 익숙하고, 봇 빌더 UI에 **React Flow** 라이브러리가 더 성숙해서 **React + Next.js가 올바른 선택**

---

### 9.2 작업 큐: BullMQ vs Celery 상세 비교

#### 아키텍처 차이
```
BullMQ (Node.js)                    Celery (Python)
─────────────────                   ─────────────────
Redis 기반                          Redis 또는 RabbitMQ 기반
Node.js 전용                        Python 전용
단순한 구조                         복잡하지만 강력한 기능
```

#### 기능 비교

| 기능 | BullMQ | Celery | 비고 |
|------|--------|--------|------|
| **기본 큐잉** | ✅ | ✅ | 둘 다 우수 |
| **스케줄링** | ✅ 크론 지원 | ✅ 크론 + 간격 | Celery가 더 유연 |
| **재시도 로직** | ✅ 지수 백오프 | ✅ 지수 백오프 | 비슷 |
| **우선순위 큐** | ✅ | ✅ | 비슷 |
| **작업 체이닝** | ⚠️ 기본적 | ✅ Canvas (강력) | Celery 우세 |
| **작업 그룹** | ⚠️ 수동 구현 | ✅ group, chord | Celery 우세 |
| **모니터링** | Bull Board | ✅ Flower | Celery 우세 |
| **분산 처리** | ✅ | ✅ 더 성숙 | Celery 우세 |
| **언어 통합** | Node.js만 | Python만 | - |

#### 우리 상황에서의 고려사항

**Backend가 FastAPI(Python)이므로:**
```
┌─────────────────────────────────────────────────────────────────┐
│  옵션 A: Celery 사용 (Python 통일)                              │
│                                                                  │
│  FastAPI ──→ Celery ──→ Redis ──→ Workers (Python)              │
│                                                                  │
│  장점:                                                          │
│  - 언어 통일 (Python)                                           │
│  - LLM 라이브러리 (langchain, openai) 직접 사용                 │
│  - 복잡한 워크플로우 (Canvas: chain, group, chord)              │
│  - 작업 정밀 제어 (retry, timeout, rate limit)                  │
│  - Flower로 모니터링                                            │
│                                                                  │
│  단점:                                                          │
│  - 설정 복잡도 높음                                              │
│  - 메모리 사용량 더 높음                                         │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│  옵션 B: BullMQ 사용 (Node.js 별도)                             │
│                                                                  │
│  FastAPI ──→ Redis ──→ BullMQ Workers (Node.js)                 │
│                                                                  │
│  장점:                                                          │
│  - 단순한 구조                                                   │
│  - 가벼움                                                        │
│  - Playwright/Puppeteer와 자연스러운 통합                       │
│                                                                  │
│  단점:                                                          │
│  - 두 가지 언어 유지 (Python + Node.js)                         │
│  - LLM 호출 시 API 통신 필요                                    │
│  - 복잡한 워크플로우 구현 어려움                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### 🔴 권장사항: Celery

**이유:**
1. **FastAPI + Celery = Python 통일** → 유지보수 용이
2. **LLM 통합**: OpenRouter/GPT 호출 로직을 Worker에서 직접 처리
3. **작업 정밀도**: 스크래핑 작업은 복잡한 체이닝 필요
   - 예: URL 수집 → 각 페이지 스크래핑 → 데이터 정제 → 내보내기
   - Celery Canvas로 `chain(crawl, scrape, process, export)` 구현
4. **스케줄링**: Celery Beat로 크론 스케줄 자연스럽게 통합

**Playwright는?**
- Python용 Playwright 있음 (`playwright-python`)
- 또는 Playwright를 별도 마이크로서비스로 분리 가능

---

## 10. Firecrawl 미사용 시 복잡성 분석

### 10.1 Firecrawl이 해결해주는 것들

| 기능 | Firecrawl 사용 | 직접 구현 시 |
|------|---------------|-------------|
| **브라우저 관리** | ✅ 내장 | ❌ Playwright 풀 직접 관리 |
| **JavaScript 렌더링** | ✅ 자동 | ⚠️ 구현 필요 |
| **프록시 로테이션** | ✅ 내장 옵션 | ❌ 직접 구축 |
| **안티봇 우회** | ✅ 일부 내장 | ❌ 직접 구현 (복잡) |
| **HTML→Markdown** | ✅ 자동 변환 | ⚠️ 라이브러리 필요 |
| **사이트맵 크롤링** | ✅ 내장 | ❌ 직접 구현 |
| **Rate Limiting** | ✅ 자동 | ⚠️ 구현 필요 |
| **재시도 로직** | ✅ 내장 | ⚠️ 구현 필요 |

### 10.2 직접 구현 시 추가 작업량

```
┌─────────────────────────────────────────────────────────────────┐
│                  직접 구현해야 할 것들                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. 브라우저 풀 관리 (예상: 2-3주)                              │
│     ├── 브라우저 인스턴스 생성/재사용                           │
│     ├── 메모리 누수 방지                                        │
│     ├── 크래시 복구                                             │
│     └── 동시 실행 제한                                          │
│                                                                  │
│  2. 안티봇 대응 (예상: 2-4주, 지속적)                           │
│     ├── User-Agent 로테이션                                     │
│     ├── 헤더 위장                                               │
│     ├── 마우스 움직임 시뮬레이션                                │
│     ├── 타이밍 랜덤화                                           │
│     └── 캡챠 처리 (외부 서비스 연동?)                           │
│                                                                  │
│  3. 프록시 시스템 (예상: 1-2주)                                 │
│     ├── 프록시 풀 관리                                          │
│     ├── 실패 시 자동 교체                                       │
│     ├── 국가별 프록시                                           │
│     └── 프록시 헬스체크                                         │
│                                                                  │
│  4. 에러 핸들링 (예상: 1주)                                     │
│     ├── 타임아웃 처리                                           │
│     ├── 네트워크 에러                                           │
│     ├── 지수 백오프 재시도                                      │
│     └── 부분 실패 복구                                          │
│                                                                  │
│  총 예상 추가 개발: 6-10주                                      │
└─────────────────────────────────────────────────────────────────┘
```

### 10.3 비용 비교

| 항목 | Firecrawl (셀프호스팅) | 직접 구현 |
|------|----------------------|----------|
| 초기 개발 | 1주 (연동) | 6-10주 |
| 유지보수 | 낮음 (업데이트 따라가기) | 높음 (직접 버그 수정) |
| 서버 비용 | 비슷 | 비슷 |
| 유연성 | 중간 | 높음 |
| 리스크 | 외부 의존성 | 기술 부채 |

### 10.4 권장사항: 단계별 접근

```
Phase 1 (MVP): Firecrawl 사용
├── 빠른 개발 (6-10주 절약)
├── 핵심 기능에 집중
└── 시장 검증 우선

Phase 2 (PMF 확인 후): 하이브리드
├── Firecrawl로 기본 처리
├── 특수 케이스만 Playwright 직접 제어
└── 점진적 마이그레이션

Phase 3 (스케일): 선택
├── A: Firecrawl 계속 (문제 없으면)
└── B: 자체 엔진 (커스터마이징 필요시)
```

**✅ 결론**: MVP에서 Firecrawl을 쓰면 **6-10주 개발 시간 절약**. 
첫 고객 확보가 우선이므로 Firecrawl로 시작하고, 필요시 마이그레이션.

---

## 11. 개발 로드맵

### Phase 1: MVP (목표: 첫 유료 결제)
```
Week 1-2: 기반 구축
├── 프로젝트 셋업 (Next.js + FastAPI)
├── Firecrawl 셀프호스팅 구축
├── 기본 인증 (이메일/비밀번호)
└── DB 스키마 설계

Week 3-4: 핵심 기능
├── iframe 기반 셀렉터 (무료 티어)
├── Chrome 확장 프로그램 (유료 티어)
├── Firecrawl + LLM 파이프라인
└── 페이지네이션 처리

Week 5-6: 자동화 & 연동
├── 스케줄러 (Celery Beat)
├── Google Sheets 연동
├── 작업 큐 (Celery + Redis)
└── 실행 로그/이력

Week 7-8: 결제 & 출시
├── 토스페이먼츠 연동
├── 사용량 추적/제한
├── 랜딩 페이지
└── 베타 출시
```

### Phase 2: 성장 (첫 결제 후)
- 글로벌 마이그레이션 (영어 UI, Stripe)
- 추가 내보내기 (Slack, Notion, Webhook)
- 템플릿 마켓플레이스
- 팀 협업 기능

### Phase 3: 최종 목표
- **Playwright MCP 기반 화면 직접 추출**
- 실시간 미리보기
- 인터랙션 녹화 → 워크플로우 자동 생성
- 고급 안티봇 우회

---

## 12. 다음 단계

### 즉시 실행
1. [ ] 경쟁사 분석 (Browse AI, Octoparse 등 직접 사용해보기)
2. [ ] Firecrawl 로컬 셋업 테스트

### 설계 문서
3. [ ] 상세 API 명세 작성
4. [ ] 데이터베이스 스키마 설계
5. [ ] UI/UX 와이어프레임

---

## ✅ 해결된 질문들

| 질문 | 결정 |
|------|------|
| **LLM 선택** | OpenRouter 무료 모델 → GPT-5 (Agent 필요시) |
| **인프라** | Vercel + Railway → 첫 고객 후 마이그레이션 결정 |
| **작업 큐** | Celery (Python 통일 + 워크플로우 체이닝) |
| **Frontend** | React + Next.js + TypeScript |
| **Backend** | FastAPI (Python) |

---

*이 문서는 설계 과정에서 계속 업데이트됩니다.*
